# -*- coding: utf-8 -*-
"""Sentiment_analyst_prediction_with_nlp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/sayid-alt/sentiment-analysis-nlp/blob/main/Sentiment_analyst_prediction_with_nlp.ipynb

dataset: [kaggle](https://www.kaggle.com/datasets/damirolejar/sentiment-analysis-nlp-trainset-data?select=final_negative.txt)

See copied project on [github](https://github.com/sayid-alt/sentiment-analysis-nlp)

# Import library
"""

import tensorflow as tf
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pickle

from sklearn.model_selection import train_test_split, StratifiedShuffleSplit

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Embedding, Flatten
from google.colab import files
from sklearn.metrics import ConfusionMatrixDisplay

import os
import zipfile
import urllib.request

"""# Utilities"""

DOWNLOAD_URL = 'https://github.com/sayid-alt/sentiment-analysis-nlp/raw/main/data/text.zip'
DOWNLOAD_DIR = os.path.join(os.getcwd(), 'datasets')

def download_data(url=DOWNLOAD_URL, dir=DOWNLOAD_DIR):
    # Downloading data
    print('Downloading data...')
    if not os.path.exists(dir):
        os.makedirs(dir)

    zip_path = os.path.join(dir, 'text.zip')
    urllib.request.urlretrieve(url, zip_path)
    print(f'Success downloading data to {zip_path}')

    # Extract zip file
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(dir)
        print(f'Success extracting data to {dir}')


# prep data from text file to variable in list type
def prep_data_txt(filepath):
  '''
    filepath: path for text file
    return: list of text
  '''
  datalist = []
  with open(filepath, 'r') as file:
    lines = file.readlines()
    for line in lines:
      datalist.append(line.strip())
  return datalist


# retrieve data
def retrieve_sentiment_data():
  '''
    return: neg_text, pos_text, ne_text as the dataset for sentiment analysis
  '''
  neg_text = prep_data_txt(os.path.join(data_dir, 'final_negative.txt')) #negative data
  pos_text = prep_data_txt(os.path.join(data_dir, 'final_positive.txt'))# positif data
  ne_text = prep_data_txt(os.path.join(data_dir, 'final_neutral.txt'))

  print(f'Negative data length: {len(neg_text)}')
  print(f'Positive data length: {len(pos_text)}')
  print(f'neutral data length: {len(ne_text)}')

  return neg_text, pos_text, ne_text

def stratified_split():
  # split data using stratified with size train/val/test 80/10/10 respectively
  sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
  train_index, test_index = next(sss.split(df['sentiment'], df['label']))

  # split throug test size for get validation split
  sss_val = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)
  val_index, test_index = next(sss_val.split(df.iloc[test_index]['sentiment'], df.iloc[test_index]['label']))

  return train_index, val_index, test_index

"""# Download data"""

download_data()

"""# Generate Data"""

data_dir = os.path.join(DOWNLOAD_DIR, 'text')
print(os.listdir(data_dir))

neg_text, pos_text, ne_text = retrieve_sentiment_data()

"""# Data Preprocessing

## Distribution

Below we merge all data into 3 sentiment categories:
- 0: Negative sentiment
- 1: Positif sentiment
- 2: neutral sentimen
"""

df = pd.DataFrame({
    'sentiment' : neg_text + pos_text + ne_text,
    'label' : ['negative'] * len(neg_text) + ['positive'] * len(pos_text) + ['neutral'] * len(ne_text)
})
df.head()

plt.figure(figsize=(5, 4))
sns.histplot(df, x='label', bins=3, color='skyblue')
plt.title('Distribution of Sentiment Labels')
plt.xlabel('Sentiment')
plt.ylabel('Frequency')
plt.show()

"""## Split data

Split data, using stratified shuffle split from sklearn []:
- Train: 80
- Val: 10
- test: 10
"""

# split data using stratified with size train/val/test 80/10/10 respectively
train_index, val_index, test_index = stratified_split()

# indexing the splitted data frame
train_df = df.iloc[train_index]
val_df = df.iloc[val_index]
test_df = df.iloc[test_index]

# check the size of each split
len_df = len(df)
len_train = len(train_df)
len_val = len(val_df)
len_test = len(test_df)

print(f'Train data size: {round(len_train/len_df,2)}')
print(f'Val data size: {round(len_val/len_df,2)}')
print(f'Test data size: {round(len_test/len_df,2)}')

split_name = ['train', 'val', 'test']
split_df = [len_train, len_val, len_test]

plt.figure(figsize=(5, 4))
bars = plt.bar(x=split_name, height=split_df, color='skyblue')

for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2.0, height - (height/2.0), '%0.2f%s' % ((height/len_df)*100, '%'), ha='center', va='bottom', color='teal')

plt.title('Distribution of Sentiment Labels')
plt.xlabel('Sentiment')
plt.ylabel('Frequency')
plt.show()

"""## Preparation for training"""

# split for training portion
X_train, y_train = train_df['sentiment'], train_df['label']
X_val, y_val = val_df['sentiment'], val_df['label']
X_test, y_test = test_df['sentiment'], test_df['label']

# X_train.to_csv('X_train.csv', index=False)
# y_train.to_csv('y_train.csv', index=False)
# X_val.to_csv('X_val.csv', index=False)
# y_val.to_csv('y_val.csv', index=False)
# X_test.to_csv('X_test.csv', index=False)
# y_test.to_csv('y_test.csv', index=False)

# try:
#   from google.colab import files
# except e:
#   print(e)
# else:
#   files.download('X_train.csv')
#   files.download('y_train.csv')
#   files.download('X_val.csv')
#   files.download('y_val.csv')
#   files.download('X_test.csv')
#   files.download('y_test.csv')

"""### Hyperparameters"""

# Vocabulary size of the tokenizer
vocab_size = 10000

# Maximum length of the padded sequences
max_length = 50

# Output dimensions of the Embedding layer
embedding_dim = 16

"""### Tokenize data"""

tokenizer = Tokenizer(num_words=vocab_size, oov_token='<OOV>')
tokenizer.fit_on_texts(X_train)
word_index = tokenizer.word_index

print(len(word_index))

label_tokenizer = Tokenizer()
label_tokenizer.fit_on_texts(y_train)
label_index_word = label_tokenizer.index_word

# start encoder label from 0
labels_encoder = {key - 1: value for key, value in label_index_word.items()}

# Tokenize the label
training_labels = label_tokenizer.texts_to_sequences(y_train)
val_labels = label_tokenizer.texts_to_sequences(y_val)
test_labels = label_tokenizer.texts_to_sequences(y_test)

print(labels_encoder)

# convert labels list to np array
# Below arrays are subtracted by one, because all index should start from 0, instead the current label before subtract started with one
training_labels = np.array(training_labels) - 1
val_labels = np.array(val_labels) - 1
test_labels = np.array(test_labels) - 1

print(training_labels.shape)
print(val_labels.shape)
print(test_labels.shape)
len(training_labels), len(val_labels), len(test_labels)

"""### Sequence data"""

sequences_train = tokenizer.texts_to_sequences(X_train)
sequences_val = tokenizer.texts_to_sequences(X_val)
sequences_test = tokenizer.texts_to_sequences(X_test)

"""### padded data"""

padded_train = pad_sequences(sequences_train, maxlen=max_length, truncating='post', padding='post')
padded_val = pad_sequences(sequences_val, maxlen=max_length, truncating='post', padding='post')
padded_test = pad_sequences(sequences_test,maxlen=max_length, truncating='post', padding='post')

print(padded_train[0], padded_train.shape)
print(padded_val.shape)
print(padded_test.shape)

"""# Build and compile Model"""

model = tf.keras.Sequential([
    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length),
    Flatten(),
    Dense(24, activation='relu'),
    Dense(3, activation='softmax')
])

model.summary()

model.compile(loss='sparse_categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'],
              )

"""# Set Callbacks"""

# early stopping
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',
                                                  mode='min',
                                                  patience=5, restore_best_weights=True)



# Model checkpoint, saving the best only val_acc

checkpoint_filepath = '/tmp/ckpt/checkpoint-{epoch:02d}-{accuracy:.2f}-{val_accuracy:.2f}.keras'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    monitor='val_accuracy',
    mode='max',
    save_best_only=False,
    verbose=1)

"""# Model training"""

num_epochs=30
history = model.fit(padded_train, training_labels, epochs=num_epochs,
                    validation_data=(padded_val, val_labels), verbose=2,
                    callbacks=[early_stopping, model_checkpoint_callback]
                    )

"""# Visualize the results"""

# Plot utility
def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()

# Plot the accuracy and loss
plot_graphs(history, "accuracy")
plot_graphs(history, "loss")

"""# Saving best checkpoint model"""

# download the model of 9-th ckpt for minimum overfitting defined.
model_to_download = '/tmp/ckpt/checkpoint-09-0.94-0.94.keras'
try:
  files.download(model_to_download)

except e:
  print(e)

"""# Evaluate the model"""

def download_model():
  model_url = 'https://github.com/sayid-alt/sentiment-analysis-nlp/raw/main/model/checkpoint-09-0.94-0.94.keras'
  # Find the position of the last forward slash '/'
  last_slash_index = model_url.rfind('/')

  # Extract the substring from the last slash to the end of the string
  model_name = model_url[last_slash_index + 1:]
  urllib.request.urlretrieve(model_url, model_name)
  print('Downloading model...')
  print('Success downloading model')
  return model_name

model_name = download_model()

'''Uncomment this line if you want to model downloaded from github repo'''
loaded_model = tf.keras.models.load_model(model_name)

'''Uncomment this for last model trained'''
# loaded_model = tf.keras.models.load_model('model.h5')
loaded_model.summary()

loss, accuracy = loaded_model.evaluate(padded_test, test_labels)
print(f'Loss: {loss}')
print(f'Accuracy: {accuracy}')

"""## Prediction"""

# predict the test labels from dataset

# show prediction softmax probability
prediction = loaded_model.predict(padded_test)
print(prediction)

"""## Confussion Matrix"""

labels_encoder

np.unique(test_labels)



# Return the highest softmax prob index
pred_labels = np.argmax(prediction, axis=1)
print(pred_labels)
con_mat = tf.math.confusion_matrix(labels=test_labels, predictions=pred_labels).numpy()
con_mat

con_mat_disp = ConfusionMatrixDisplay(con_mat, display_labels=['Positive','Neutral', 'Negative'])
con_mat_disp.plot(cmap=plt.cm.Blues)
plt.show()

"""## Input user"""

user_input = input('Enter a sentence: ')

# tokenize the sentence
user_sequence = tokenizer.texts_to_sequences([user_input])

# pad the sequence
user_padded = pad_sequences(user_sequence, maxlen=max_length, truncating='post', padding='post')

# predict the sentiment
prediction = loaded_model.predict(user_padded)

# -- - Summary prediciton ---
print('---- Prediction summary -----')
print(labels_encoder)
print('Softmax probability:', prediction)

# return the highest probability of 3 categories
predicted_index = np.argmax(prediction, axis=1)

# get the predicted label based on highest probability
predicted_label = labels_encoder[predicted_index[0]]
print(f'Predicted sentiment: {predicted_label}')

"""# Deployment

## Using Pickle
"""

with open('model.pkl', 'wb') as file:
  pickle.dump(loaded_model, file)

files.download('model.pkl')